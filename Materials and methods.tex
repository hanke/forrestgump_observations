\section*{Materials and methods} 

\subsection*{Stimulus}

The annotated stimulus was a slightly shortened version of the movie
``Forrest~Gump'' (R.~Zemeckis, Paramount Pictures, 1994). Two different
variants of this movie were annotated separately. The first one was a German
audio-description as broadcast as an additional audio track for visually
impaired listeners on Swiss public television (Koop, Michalski, Beckmann,
Meinhardt \& Benecke produced by Bayrischer Rundfunk, 2009). This audio-only
stimulus was the same as the one used in~\cite{HBI+14}, and is largely
identical to the dubbed German soundtrack of the movie except for interspersed
narrations by a male speaker who describes the visual content of a scene. These
descriptions take place when there is no dialog, off-screen speech, or other
relevant audio-content in the movie. The second variant is the audio-visual
movie with the original dubbed German soundtrack (without additional
narrations) and the exact same timing (and cut scenes) as the audio-only
version \cite[contains instructions on how to reproduce the stimulus from the
DVD release]{HBI+14}.

\subsection*{Observers}

Annotations were created by the authors themselves as part of a practical
course on scientific observation. A total of 12 students at the
Otto-von-Guericke-University in Magdeburg, Germany participated in this effort
and received course credit.  Based on personal preference, students assigned
themselves to one of two groups tasked with the annotation of portrayed emotion
in either the audio-visual movie, or the audio-only version. The audio-visual
group consisted of \AVTotalRaters\ students (all female), the audio-only group
comprised \AOTotalRaters\ students (all male). This gender bias could have had
an impact on the perceived emotions.  None of the observers participated in the
previous brain imaging study~\cite{HBI+14}.

\subsection*{Procedure}

Annotations were performed with the help of Advene\cite{AP2005}, a dedicated
open-source video annotation tool that offers convenient navigation of
structural units of the stimulus and provided uniform position information with
sub-second precision.  Each observer annotated all of 205 previously identified
movie scenes in an individual random order to minimize ``carry-over'' effects
and help observers focus on current indicators of portrayed emotions in
contrast to more slowly changing characteristics, such as mood, or other biases
introduced by the movie plot. Scenes were used as units for annotation, because
transitions between them typically involve a change of location or fast
progression of time that make persistence of the nature of portrayed emotions
across scenes less likely.

Observers were instructed to work alone in a setting free of distractions, and
to use headphones for optimal saliency of the stimulus. There was no strict
guideline regarding the length of an annotation session, but observers were
informed to stop working for any number of breaks whenever they could no longer
guarantee a high level of attention for the task. Sessions were typically
spread over multiple days over the course of three weeks. On average, observers
required around 30 hours each to complete the annotation.

\hl{TODO we need 2-3 sentences explaining the workflow between advene and the
spreadsheet application. Most readers probably do not know advene. What did
advene contribute? How often did you look at a scene? once? twice? once per
variable?}

\subsection*{Annotation content}

Each observer collected annotations in a spreadsheet application with columns
for start time and end time of an emotion, a label of the movie character
portraying the emotion, variables for valence, arousal and direction of the
emotion, as well as an optional emotion category label, and a list of
identified indicators for the start and end of an emotion. Each of these
variables are described in more detail in the remainder of this section. In
addition, observers also recorded the index of the scene containing a
respective emotion. This variable was only used for error detection (mismatch
of emotion duration with the start and end time of a scene) and is not included
in the published dataset.

\paragraph{Start and end time}

The start time identifies the onset of the first observed indicator of an
emotion. Likewise, the end time corresponds to the offset of an emotion,
defined as the time where no evidence for an emotion is present anymore.
Observers were instructed to aggregate short segments of uniform emotional
episodes into a single annotation. For example, if a character was happy and
smiling throughout a long period in a scene, the episode would span all the
time from the beginning to the end of the smile even if the character's face was
not visible the entire period, as long as no evidence of a change of emotion
was found.  All times are reported in seconds. Although Advene offers time
stamps with the precision of the movie frame rate (25 frames per second),
pre-tests revealed that the temporal accuracy of human observers for complex
emotion indicators, like a developing facial expression, is much coarser. In
order to reduce the chance of unrecoverable typos in the time stamps, observers
were instructed to record timing information with second-precision only.

\paragraph{Onset and offset cues}

Onset indicators describe what kind of evidence for an emotion was detected in
the movie stimulus. We distinguished facial expressions, gestures or body
language, context information, verbal and non-verbal audio cues. Observers
could record multiple onset indicators. Despite the term ``onset'' these
indicators did not all have to be present at the very beginning of an emotional
episode. For example, an extended period of sadness could start with a facial
expression and later on add a congruent body language cue. In that case the
respective labels were aggregated into a list. In the audio-movie stimulus the
narrator was frequently the only source of information regarding the emotional
state of a character, hence we included a dedicated category for this scenario.

In addition to onset indicators, observers also recorded the type of evidence
for the end of an emotional episode. Four conditions were distinguished: 1) the
change from one emotion to another, 2) entering a neutral emotional state, 3) a
character leaving an ongoing scene with no further evidence for its emotional
state, and 4) the end of a scene.  Table \ref{tab:onoffset_indicators}
describes all distinguished cue categories.

\begin{table*}
  \centering
  \begin{tabular}{lp{10cm}}
    \textbf{Label} & \textbf{Description} \\
    \\\hline\\
    \textit{Onset}\\
    AUDIO & Non-verbal audio cues, feelings expressed without words, for example laughing, crying, wheezing, sighing\\
    CONTEXT & Emotion portrayed through context information and not directly by a character, for example a social rejection of an individual by a group without visible emotional response\\
    GESTURE & Body language cues \\
    FACE & Facial expressions \\
    NARRATOR & Description of an emotion given by the narrator (only for audio stimulus)\\
    VERBAL & Emotion expressed through words either as a direct description (e.g. a self-report) or by means of a particular choice of words (e.g. harsh language)\\
    \\\hline\\
    \textit{Offset}\\
    EMOCHANGE & Present emotion is replaced by another, different emotion \\
    EMOFADED & Emotion fades with a change to an emotion neutral state \\
    CHARLEFT & Character leaves a scene (no further evidence for an emotion)\\
    SCENEEND & Scene transition with a change of time or location indicates the end of an emotion\\
    \\\hline

  \end{tabular}
  \caption{Categories for cues indicating the onset or offset of an emotion.}
  \label{tab:onoffset_indicators}
\end{table*}



\paragraph{Character label}

This label identifies which movie character is portraying an emotion. Observers
were provided with a list of character labels that was derived from an
annotation of the movie's dialog in order to achieve a consistent labeling.
However, some characters portray emotions but have no lines of dialog, in other
instances observer chose inconsistent labels. During data curation character
labels were therefore manually consolidated into a set of 35 categories that
uniformize labeling across observers. The full list of character labels is
shown in table \ref{tab:characters}. The 35 character labels were derived using
the following rules: a) main characters and famous/familiar characters receive
individual labels, as do the two narrator voices; b) relatives of main
characters are labeled according to their relation; c) all other characters are
consolidated into generic categories that preserve gender, age (children,
adults, or seniors), and number (individual, or group).

\begin{table*}
  \centering
  \begin{tabular}{p{4cm}p{4cm}p{3.6cm}p{4cm}}
    \textbf{Character label} & \textbf{Description} & \textbf{Character label} & \textbf{Description} \\
\\\hline\\

\multicolumn{4}{l}{\textit{Main characters}} \\
BUBBA     & Forrest's army pal & FORRESTJR & young Forrest Gump \\
DAN       & Lt.~Dan & JENNY     & Forrest's love \\
FORREST   & adult Forrest Gump &           & \\
\\\hline\\

\multicolumn{4}{l}{\textit{Special}} \\
FORRESTVO & voice-over narrator Forrest & NARRATOR & audio-description narrator \\
\\\hline\\

\multicolumn{4}{l}{\textit{Famous characters}} \\
 ABBIEHOFFMAN     & activist (lookalike actor)                   & NEILARMSTRONG    & first man on the moon (orig.~footage) \\ 
 BOBHOPE          & entertainer (orig.~footage)                  & PRESIDENTJOHNSON & US-president (orig.~footage) \\
 DICKCAVETT       & talk master (orig.~footage)                  & PRESIDENTKENNEDY & US-president (orig.~footage) \\
 ELVISPRESLEY     & The King (orig.~footage and lookalike actor) & PRESIDENTNIXON   & US-president (orig.~footage)  \\
 JOHNLENNON       & Beatle (orig.~footage)                       & ROBERTKENNEDY    & Brother of J.~F.~Kennedey (orig.~footage) \\                  & \\
\\\hline\\

\multicolumn{4}{l}{\textit{Relatives of main characters}} \\
BUBBASGRGRANDMA   & Bubba's great-grandmother & JENNYSGRANDMA & Jenny's grandmother\\
BUBBASGRGRGRANDMA & Bubba's great-great-grand\-mother & MRSBLUE       & Bubbas's mother\\
BUBBASYOUNGBROTHER      & Bubba's young brother & MRSGUMP       & Forrest's mother \\
JENNYSDAD               & Jenny's father &              & \\
\\\hline\\

\multicolumn{4}{l}{\textit{Generic character categories}} \\
CHILDREN  & group of children & MEN      & group of men \\
GIRL      & individual girl & OLDMEN   & group of old men \\
BOY       & individual boy & WOMAN    & individual woman \\
OLDERBOYS & group of older boys & OLDWOMAN & old woman\\
MAN       & individual man & WOMEN    & group of women \\
OLDMAN    & individual old man & CROWD    & group of people with mixed gender \\
 \\\hline
  \end{tabular}
  \caption{Consolidated character labels. Any annotation of a portrait emotion
  is associated with exactly one of these 36 labels. While main characters,
  their relatives, and famous characters have individual labels, all remaining
  characters in the movie are aggregated into generic categories that preserve
  information on gender, age group and number.}
  \label{tab:characters}
\end{table*}

It is important to note that only emotions portrayed by one or more movie
character where recorded. This excludes other potentially emotion-inducing
stimuli, such as music in the soundtrack, or inanimate stimuli, like the
depiction of a beautiful sunset, or chirping birds.

\paragraph{Valence, arousal and direction}

Binary variables were used to record the valence of an emotion (positive vs.
negative), the state of arousal of the character portraying it (high vs. low),
and whether the emotion is directed towards the character itself or another one
(e.g. feeling pity for somebody). Together, these three indicator variables offer a
coarse categorization of observed emotions in the spirit of dimensional models
of emotion.

Arousal was considered a global indicator of the emotional status of a
character, i.e. for any given time in the movie annotations either indicate a
high level of arousal for a character or a low level, but never both at the
same time. Unlike arousal, valence and direction indicators were considered
non-exclusive, i.e. a character could show signs of positive and negative
emotions at the same time. Likewise a character could exhibit self-directed
emotions, and emotions directed towards others simultaneously. In order to
annotate such situations, observers had to create records for two or more
portrayed emotions with overlapping timing.

\paragraph{Emotion labels}

Observers were instructed to assign one of 22 labels for discrete emotion
categories. These categories were derived from \cite{Ort1990} and concrete
definitions were developed based on the Specific Affect Coding System
\cite{CG2007}. The complete list of emotions is shown in table
\ref{tab:emotion_categories}.  Pre-tests using a larger and more detailed set
of emotion categories revealed weak agreement between observers for a forced
categorization of emotional episodes. Based on these results we decided to make
this part of the annotation optional and observers were instructed to assign an
emotion label only if they saw a ``perfect'' match with any of the categories.

\begin{table*}
  \centering
  \begin{tabular}{lp{10cm}}
    \textbf{Label} & \textbf{Description} \\
    \\\hline\\
    ADMIRATION & Appreciation of another person or object \\
    ANGERRAGE & Response to injured respect or autonomy of a person by another one \\
    COMPASSION & Concern in someone's suffering \\
    CONTEMPT & Disdain of another person or object \\
    DISAPPOINTMENT & When expectations were not met \\
    FEAR & Triggered by a dangerous situation \\
    FEARCONFIRMED & \hl{TODO: missing description}\\
    GLOATING & To be happy because another person suffers \\
    GRATIFICATION & Triggered by a positive event which one owes by themselves  \\
    GRATITUDE & Triggered by a positive event which one owes by another person \\
    HAPPINESS & Pleasant and joyful condition \\
    HAPPYFOR & concern in happiness of someone else \\
    HATE & Rejection of another person, intense dislike, or animosity \\
    HOPE & Positive expectation for a person or special event in the future \\
    LOVE & Affection to another person \\
    PRIDE & Positive reflection of accomplishments and status \\
    RELIEF & A worry/negative expectation do not come to pass \\
    REMORSE & Triggered by a negative event initiated by themselves \\
    RESENTMENT & Response to perceived unfair treatment \\
    SADNESS & Emotional pain after loss or misfortune \\
    SATISFACTION & When expectations get fulfilled \\
    SHAME & Dissatisfaction with one's own behavior, because norms are violated \\
    \\\hline
  \end{tabular}
  \caption{List of 22 emotion categories observer could (optionally) use to
    futher qualify the nature of a portrayed emotion. \hl{TODO: doublecheck descriptions}}
  \label{tab:emotion_categories}
\end{table*}

\subsection*{Quality assurance}

We used an automated procedure to check annotation records of individual
observer for errors, or potential problems. Observers submitted their
annotations in tabular form to a script that generated a list of error and
warning messages. Using this feedback, observers double-checked their
annotations as often as necessary until no objective errors were found and
all warning messages were confirmed to be false positives. The following
annotation properties were tested:

\begin{itemize}
  \item start time had to precede end time (error)
  \item no character label (error)
  \item no onset cue label (error)
  \item missing arousal, valence, or direction value (error)
  \item conflicting simultaneous arousal annotation for the same character (error)
  \item emotion portrayed for longer than a minute (warning)
  \item annotation starts prior to a scene start or ends after a scene end
    (warning)
  \item unrecognized code/label for any variable (warning)
  \item no offset cue label (warning)
\end{itemize}

\subsection*{Dataset content}

The release data comprises four components: raw annotations, aggregate
inter-observer agreement (IOA) time series, emotion episode segmentations, and
the source code to generate all data derived from the raw annotations, as well
as all summary statistics presented in the Data Note.

\paragraph{Raw annotations} In order to maximize the utility of the annotations
they are released in full, and not only as aggregate information. The
annotations from each observer are available in an individual plain text file
with comma-separated value (CSV) markup in the \texttt{raw} directory. This
file format facilitates import and further processing in virtually any software
application that supports data in tabular form. The file names indicate the
annotated stimulus type (prefix \texttt{av} for the audio-visual movie and
\texttt{ao} for the audio-only movie), as well as the observer identity.

Each file contains nine columns that correspond to the annotation properties
described above. \texttt{start}, \texttt{end})~start and end time of an emotion
episode, reported in seconds from the start of the movie; \texttt{character})~a
single character label from the set shown in table \ref{tab:characters}
indicating the character portraying the emotion; \texttt{arousal})~state of
arousal indicated with the labels \texttt{LOW} or \texttt{HIGH};
\texttt{valence})~emotional valence indicated with the label \texttt{POS} or
\texttt{NEG}; \texttt{direction})~direction of the emotion indicated with the
labels \texttt{SELF} or \texttt{OTHER}; \texttt{emotion})~a space-separate list
of emotion labels from the set shown in table \ref{tab:emotion_categories};
\texttt{oncue})~a space-separated list of onset indicators for a portrayed
emotion from the set shown in table \ref{tab:onoffset_indicators}; and
\texttt{offcue})~a space-separated list of offset indicators from the set also
shown in table \ref{tab:onoffset_indicators}.  With the exception of the columns
\texttt{emotion} and \texttt{offcue} all variables are considered mandatory
and are present for all annotations. The remaining columns contain optional
attributes and can be empty.

\paragraph{Inter-observer agreement (IOA) time series} IOA time series reflect
the consistency of observations at any particular time in the movie. The time
series amplitude corresponds to the fraction of observers indicating the
presence of a particular attribute of an emotion eposide (interval [0,1]). Time
series for the three bipolar attriutes \textit{arousal}, \textit{valence}, and
\textit{direction}, were computed by subtracting the IOA with respect to the
presence of the both extremes from each other.  For example, the time series
for \textit{arousal} was computed by subtracting the time series of the IOA for
low-arousal episodes from the time series of high-arousal episodes. This
results in values from an interval of [-1,1], where -1 corresponds to perfect
agreement with respect to the presence of a low-arousal episode, and +1
indicating perfect agreement regarding high arousal.

IOA time series were computed for three different segment durations (i.e.
agreement is considered when observers indicate the presence of a particular
attribute anywhere in a segment): \unit[1]{s} (temporal precision of the
annotations), \unit[2]{s} (corresponding sampling rate of the available brain
imaging data), and the actual location and duration of individual shots in the
movie (median duration $\approx$\unit[5]{s}). Individual time series for the
three bipolar attributes, all 22 emotion categories, and the six emotion onset
cues are available in the \texttt{timeseries} directory. The file names are
encode as follows: prefix \texttt{ioats}, followed by a label for the segment
duration (e.g. \texttt{1s}), following by the stimulus label (\texttt{av} or
\texttt{ao}), and a character label (e.g. \texttt{forrest}, see
table~\ref{tab:characters}) or \texttt{allchar} for an aggregate time series
across all characters. All files are in plain text format with CSV markup. The
column headers indicate the corresponding emotion attribute.

\paragraph{Emotion episode segmentation}

Based on IOA time series a segmentation of the movie into episodes of portrayed
emotion was performed for each of the 35 character categories individually. IOA
time series for 33 attributes (arousal (bipolar), valence and direction
(separate for each state), 22 emotion category labels, and six onset cues) were
binarized by thresholding with a minimum absolute IOA of 50\%.  An episode of
portrayed emotion was defined as a time window with at least one super
threshold emotion attribute. The duration of an episode was determined by the
number of consecutive movie segment without a change in the unique combination
of super threshold attributes.

Emotion episodes for all characters are available in the \texttt{segmentation}
directory in plain-text files with tab-separated value (TSV) markup. The file
formats follows the conventions required by the Advene software for
``simple-structured content'', but are also compatible with generic spreadsheet
applications. Each file contains three columns (without a header). The two
columns contain the start and end time of an episode in seconds. The third
column contains a space-separated list of key/value pairs. Five such properties
are present for each episode: \texttt{char}) the label of the character
portraying the emotion; \texttt{tags}) a comma-separated list of tags
corresponding to all super-threshold emotion attributes; the three remaining
properties \texttt{arousal}, \texttt{val\_pos}, and \texttt{val\_neg}
corresponds to the median IOA value across the entire episode for arousal
(bipolar), positive and negative valence (each unipolar). The names of all tags
typically correspond to the lower-case name of the attribute, with the
exception of the three bipolar attributes, where each extreme is coded with a
dedicated label (arousal: \texttt{ha/la} (high/low); valence: \texttt{pos/neg};
direction: \texttt{self/other}.

Emotion episode segmentation was performed separately for the audio-visual and
the audio-only movie, and once with IOA time series sampled with a \unit[1]{s}
segment size and a second time sampled with the actual duration and location of
shots in the audio-visual movie. File names encode these conditions as follows:
prefix \texttt{emotions} followed by the stimulus label (\texttt{av} or
\texttt{ao}), followed by the segment size label (\texttt{1s} or
\texttt{shots}).


\paragraph{Source code}

The full source code for all descriptive statistics and figures included in
this paper is available in \texttt{descr\_stats.py} (Python script). Moreover,
this script also contains all functionality to generate the IOA time series, as
well as the emotion episode segmentation from the raw annotations in the data
release.


\subsection*{Time stamp conversion}

To be able to use the provided emotion annotations for an analysis of
functional data (fMRI, cardiac trace, etc.) acquired using the
procedure described in~\cite{HBI+14} timing information has to be converted,
because data were recorded in eight sessions using partially overlapping
segments of the movie. Table \ref{tab:timing} specifies the location of these
segments with respect to the timing of the unsegmented movie used for emotion
annotation. Subtracting movie segment start times from annotation time stamps
yields valid relative time stamps with respect to the time series of the
available functional data.

\begin{table}
  \centering
  \begin{tabular}{lrrrr}
    \textbf{\#}  & \textbf{Start}  & \textbf{End}    & \textbf{Duration}& \textbf{Boundary} \\
  \\\hline\\
   0  & 0.0    & 902.0  & 902.0  & 891.2    \\
   1  & 886.0  & 1768.0 & 882.0  & 1759.2   \\
   2  & 1752.0 & 2628.0 & 876.0  & 2618.8   \\
   3  & 2612.0 & 3588.0 & 976.0  & 3578.5   \\
   4  & 3572.0 & 4496.0 & 924.0  & 4488.0   \\
   5  & 4480.0 & 5358.0 & 878.0  & 5349.2   \\
   6  & 5342.0 & 6426.0 & 1084.0 & 6418.2   \\
   7  & 6410.0 & 7086.0 & 676.0  & 7085.5   \\
   \\\hline
  \end{tabular}

  \caption{Position of the fMRI stimulus segments with respect to the movie
    annotation time. The last column lists the position of the scene boundary
    used as a reference for the segment transition. All times are in seconds
    and refer to the full (unsegmented) stimulus, which is a shorted version of
    the original movie \cite{HBI+14}.}

  \label{tab:timing}
\end{table}

%\paragraph{Jokes}
%Finally, we also annotated jokes during the movie, defining them as scenes or lines that were intended to be funny and make the audience laugh. Jokes were annotated only in their duration and marked with the abbreviation JOK in the emotion column. 
